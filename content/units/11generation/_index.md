---
title: "Unit 11: Generation"
date: 2022-03-21
weight: 10
---

We've seen models that *classify* images and text, then more recently models that can generate one single token. What if we want to generate whole articles? Or images? Music? Programs? We can adapt the same basic approaches that we used already, but with interesting twists... and, I must admit, the results are fun.

By the end of this week you should be able to answer the following questions:

- TODO

## Preparation

A basic introduction to *decoding*:

[How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate)

Now, how do you control *what* gets generated? Choose your favorite modality and skim a very recent paper:

- Controlling generated text
  - [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190); extension: [Control Prefixes for Text Generation](https://arxiv.org/abs/2110.08329)
- Captioning images
  - [Multimodal Few-Shot Learning with Frozen Language Models](https://arxiv.org/abs/2106.13884); extension: [MAGMA -- Multimodal Augmentation of Generative Models through Adapter-based Finetuning](https://arxiv.org/abs/2112.05253)
- Generating images
  - [GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models | Abstract](https://arxiv.org/abs/2112.10741)

### Supplemental Material

- [ML and NLP Research Highlights of 2021](https://ruder.io/ml-highlights-2021/)
- [Some slides]([cs224n-2019-lecture15-nlg - cs224n-2019-lecture15-nlg.pdf](https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture15-nlg.pdf))

## Class Meetings

### Monday

### Wednesday

### Friday: Guest Lecture
