---
title: "Unit 7: Learned Representations (Embeddings)"
date: 2022-02-21
weight: 7
---

In this unit we introduce one of the most powerful concepts in machine learning: the *embedding*. It's the idea that instead of being explicitly given a representation for something, we *learn* it from data *based on the properties it should have*. We've actually already seen embeddings of various kinds (e.g., the inputs to the last layer of an image classifier for one; you could even think of the "scores" in a logistic regression model as a kind of embedding since we don't directly specify them), but we'll look at two examples where the embedding aspect is even clearer: movies and words.

Students who complete this unit will demonstrate that they can:

- Identify some examples of data augmentation and regularization.
- Predict the effect of data augmentation and regularization on model training.
- Implement a multi-layer neural network using basic numerical computing primitives

## Preparation

The fastai course videos are still a bit disorganized, sorry about that.

- Read {{<fastbook num="8" nbname="08_collab.ipynb">}}.
- Watch {{<fastvideo num="6">}} from 1h30m to the end.

### Supplemental Materials

## Class Meetings

### Monday

### Wednesday

- Discussion summary (come prepared to contribute)

### Friday

### Homework
