**Outcome**: Excellent!
**Outcome**: Meets expectations.
**Outcome**: needs Revision.

## Regression

- looks good.
- `X` is the independent variables, `y` the dependent. (this terminology is more common in a statistics setting)
- in ML, we call the columns of `X` the *features* or *predictors*, and `y` the *target*.
- Nice that you looked at the MSE and MAE.
- The parallel lines of the linear regression are *contours* of the prediction, which is actually smooth (in fact, too smooth.) Imagine you've colored in the contour lines of a topographic map.
- The descriptions of the plots are not clear. Notice how some of the boundaries are strictly horizontal/vertical while others are not. Notice how some boundaries are sharp while others are not.
- Why might the Random Forest give lines like the tree, but less sharp lines? Answer: it's the average of a bunch of trees, each of which has those sharp lines, but different ones.
- the decision tree may not be balanced in the data-structures sense.

## Classification

- great!
- good. (could be clearer, but meets expectations.)
- unclear explanations; see if this helps:
- incomplete.
- (Revise) oops: *accuracy* is good if it's *high*, so logistic regression is *worst*.
- This affects your answer to Q4 also.
- The decision tree is *overconfident*.
- The log loss (cross-entropy) considers not just the classifier's decision but the *confidence* of that decision. The decision tree was **confidently wrong** so it got high log loss.
- Logistic regression predicted a probability of about 1/3 for everything, so the probability it gave to what turned out to be the right answer was almost always about 1/3. Hence the log loss value.
- The decision tree was the classifier that *overfit*. It does really well on the training set but overly relied on happenstance features of that training set. The random forest avoided overfitting nearly as much by averaging many different predictions.
- Decision trees *can* handle probabilistic predictions just fine, just that this one didn't. If we had kept the tree from putting each house in its own leaf (using the `max_depth` parameter, for example), it would have made less confident predictions because each leaf node would contain a mixture of houses in different categories.
- The logistic regression *underfit*: it was not able to capture meaningful patterns even in the training set.
- Logistic regression fails to capture any patterns because it's basically only able to use a single dividing line (strictly speaking, a dividing plane) between each class. There's no single line that really separates low from high price here.
- While the high cross-entropy is a symptom of overfitting, that was not sufficient by itself to determine if underfitting or overfitting occurred. For that, we need to compare the training set and test set performance (the optional extension).

## Overall

- Restart and Run All before submitting. Your notebooks include outputs that don't correspond to the code.
