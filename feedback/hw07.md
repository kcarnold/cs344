**Outcome**: Excellent!
**Outcome**: Meets expectations.
**Outcome**: needs Revision.

## Regression

- looks good.
- some explanations were unclear; see if this helps:
- `X` is the independent variables, `y` the dependent. (this terminology is more common in a statistics setting)
- in ML, we call the columns of `X` the *features* or *predictors*, and `y` the *target*.
- Nice that you looked at the MSE and MAE.
- The dots are the training datapoints themselves, so they're identical for the three plots.
- The parallel lines of the linear regression are *contours* of the prediction, which is actually smooth (in fact, too smooth.) Imagine you've colored in the contour lines of a topographic map.
- The descriptions of the plots are not clear. Notice how some of the boundaries are strictly horizontal/vertical while others are not. Notice how some boundaries are sharp while others are not.
- Why might the Random Forest give lines like the tree, but with lines that are less sharp? Answer: it's the average of a bunch of trees, each of which has those sharp lines, but different ones.
- The random forest actually performed better than the decision tree; look at the numbers and think about why.
- the decision tree may not be balanced in the data-structures sense.

## Classification

- great!
- good. (could be clearer, but meets expectations.)
- missing explanation of the cross-entropy values.
- incorrect on a few points; see if this helps:
- unclear/incomplete explanations; see if this helps:
- incomplete.
- (Revise) oops: *accuracy* is good if it's *high*, so logistic regression is *worst*.
- This affects your answer to Q4 also.
- The decision tree is *overconfident*.
- The log loss (cross-entropy) considers not just the classifier's decision but the *confidence* of that decision. The decision tree was **confidently wrong** so it got high log loss. (This also suggests that it overfit the training data, but we'd need to look at the training set performance to be sure.)
- Logistic regression predicted a probability of about 1/3 for everything, so the probability it gave to what turned out to be the right answer was almost always about 1/3. Hence the log loss value.
- The decision tree was the classifier that *overfit*. It does really well on the training set but overly relied on happenstance features of that training set. The random forest avoided overfitting nearly as much by averaging many different predictions.
- Decision trees *can* handle probabilistic predictions just fine, just that this one didn't. If we had kept the tree from putting each house in its own leaf (using the `max_depth` parameter, for example), it would have made less confident predictions because each leaf node would contain a mixture of houses in different categories.
- The logistic regression *underfit*: it was not able to capture meaningful patterns even in the training set.
- Logistic regression fails to capture any patterns because it's basically only able to use a single dividing line (strictly speaking, a dividing plane) between each class. There's no single line that really separates low from high price here.
- While the high cross-entropy is a symptom of overfitting, that was not sufficient by itself to determine if underfitting or overfitting occurred. For that, we need to compare the training set and test set performance (the optional extension).

## Overall

- Restart and Run All before submitting. Your notebooks include outputs that don't correspond to the code.
