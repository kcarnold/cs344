Ok (Meets expectations).

Missing reflections on doing the tutorials. I'm assuming you did and just forgot to write about it.

I'm glad the tutorials could reinforce things we'd studied.  (I would have liked to hear more specifics about what concepts were most helpful to review etc.)

It was hard to distinguish between what was the question and what was your response in the Fundamentals analysis.

Q1: tokens, not words.
Q2: do we always need to pick the *most likely*?

The model is already assigning numbers that you could interpret as importance weights.

Q2: the model is already taking the left context into account. If we wanted it to aim for some specific future word, then you're right that we would need to change something about the model. If we just choose a different token instead of the single most likely one, then we can feed that new phrase back into the model and ask for the next word again.

predict_next_tokens lacked comments.