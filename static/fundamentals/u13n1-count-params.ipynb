{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why so big? Counting parameters in sequence models\n",
    "\n",
    "PaLM has 540 billion parameters. What could they possibly all be doing? Let's figure out where the parameter budget in sequence models goes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "def num_parameters(model):\n",
    "    \"\"\"Count the number of trainable parameters in a model\"\"\"\n",
    "    return sum(param.numel() for param in model.parameters() if param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the same input setup as last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This will be the input to a language model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tensor = torch.tensor([[ord(x) for x in sentence]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = sentence_tensor[:, 1:]\n",
    "input_ids = sentence_tensor[:, :-1]\n",
    "assert input_ids.shape == targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A big chunk of the parameters of a model come from the *word embeddings*. Let's see an example.\n",
    "\n",
    "We'll use a vocabulary size of 256 (very small, but enough to store individual *bytes*) and an embedding dimensionality of 5 (also *very* small).\n",
    "\n",
    "(Note: we'll still call these *word* embeddings even though we'll actually use them as embeddings for individual *characters*.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_vocab = 256\n",
    "emb_dim = 5\n",
    "embedder = nn.Embedding(n_vocab, emb_dim)\n",
    "embedder.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many parameters are needed for the word embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1280"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_parameters(embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn**: write a function that takes the vocabulary size and embedding dimensions and returns the number of parameters. Do this without instantiating an `nn.Embedding`; just use multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_params_for_embedding(n_vocab, emb_dim):\n",
    "    return ...\n",
    "\n",
    "assert (\n",
    "    num_params_for_embedding(n_vocab, emb_dim)\n",
    "    == num_parameters(nn.Embedding(n_vocab, emb_dim))\n",
    ")\n",
    "assert (\n",
    "    num_params_for_embedding(50000, 2048)\n",
    "    == num_parameters(nn.Embedding(50000, 2048))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete but vacuous model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now define a model that has the outward structure of a language model, but without any internal processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1280"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BareBonesLM(nn.Module):\n",
    "    def __init__(self, n_vocab, emb_dim, tie_weights=True, bias=False):\n",
    "        super().__init__()\n",
    "        self.word_to_embedding = nn.Embedding(n_vocab, emb_dim)\n",
    "        self.lm_head = nn.Linear(emb_dim, n_vocab, bias=bias)\n",
    "\n",
    "        if tie_weights:\n",
    "            assert self.lm_head.weight.shape == self.word_to_embedding.weight.shape\n",
    "            self.lm_head.weight = self.word_to_embedding.weight\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        input_embeds = self.word_to_embedding(input_ids)\n",
    "        x = input_embeds # model would go here\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "num_parameters(BareBonesLM(n_vocab=n_vocab, emb_dim=emb_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your turn: how many parameters does this have when `bias=True`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_parameters(BareBonesLM(n_vocab=n_vocab, emb_dim=emb_dim, bias=...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many parameters does it have when `tie_weights=False`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2560"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_parameters(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron\n",
    "\n",
    "Now let's start putting a model in there. Let's start with the simple MLP that we looked at in previous Fundamentals. We'll define it as a PyTorch `Module`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, emb_dim, n_hidden):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_features=emb_dim, out_features=n_hidden),\n",
    "            nn.ReLU(), # or nn.GELU() or others\n",
    "            nn.Linear(in_features=n_hidden, out_features=emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "num_parameters(MLP(emb_dim=emb_dim, n_hidden=16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn**: Write a function that returns the number of parameters for this model. Again, don't instantiate it, just use multiplication and addition. (Don't forget the biases.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_parameters_for_mlp(emb_dim, n_hidden):\n",
    "    return ...\n",
    "num_parameters_for_mlp(emb_dim, 16)\n",
    "\n",
    "assert (\n",
    "    num_parameters_for_mlp(emb_dim, n_hidden=16)\n",
    "    == num_parameters(MLP(emb_dim=emb_dim, n_hidden=16)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Language Model with MLP\n",
    "\n",
    "Now let's put that into an LM. Notice what's similar and what's different between this and the `BareBonesLM` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1461"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForwardLM(nn.Module):\n",
    "    def __init__(self, n_vocab, emb_dim, n_hidden, tie_weights=True):\n",
    "        super().__init__()\n",
    "        self.word_to_embedding = nn.Embedding(n_vocab, emb_dim)\n",
    "        self.model = MLP(emb_dim=emb_dim, n_hidden=n_hidden)\n",
    "        self.lm_head = nn.Linear(emb_dim, n_vocab, bias=False)\n",
    "\n",
    "        if tie_weights:\n",
    "            assert self.lm_head.weight.shape == self.word_to_embedding.weight.shape\n",
    "            self.lm_head.weight = self.word_to_embedding.weight\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        input_embeds = self.word_to_embedding(input_ids)\n",
    "        x = self.model(input_embeds)\n",
    "        return self.lm_head(x)\n",
    "\n",
    "\n",
    "ff_lm = FeedForwardLM(n_vocab=n_vocab, emb_dim=emb_dim, n_hidden=16)\n",
    "num_parameters(ff_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn**: Count the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_parameters_for_mlp_lm(n_vocab, emb_dim, n_hidden):\n",
    "    return (\n",
    "        num_parameters_for_mlp...\n",
    "        +\n",
    "        num_params_for_embedding...\n",
    "    )\n",
    "assert (\n",
    "    num_parameters_for_mlp_lm(n_vocab=n_vocab, emb_dim=emb_dim, n_hidden=16)\n",
    "    == num_parameters(ff_lm)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "We're going to implement an oversimplified Transformer layer. If you're interested, here are a few reference implementations of the real thing:\n",
    "\n",
    "- [Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "- PyTorch's builtin implementation: [`TransformerEncoderLayer`](https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#TransformerEncoderLayer) and [`MultiheadAttention`](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention).\n",
    "- [minGPT](https://github.com/karpathy/minGPT/blob/master/mingpt/model.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the *self-attention layer*. The biggest way it's oversimplified is that it's *single-head attention*. In practice, Transformers use *multi-head* attention, which makes many (8, 16, 48, ...) queries instead of just one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 43, 5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BareBonesSelfAttention(nn.Module):\n",
    "    '''Implements *single-head* attention, no masking, no dropout, no scaling, no init'''\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.get_query = nn.Linear(emb_dim, emb_dim)\n",
    "        self.get_key = nn.Linear(emb_dim, emb_dim)\n",
    "        self.get_value = nn.Linear(emb_dim, emb_dim)\n",
    "        self.to_output = nn.Linear(emb_dim, emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n_batch, seq_len, emb_dim = x.shape\n",
    "\n",
    "        # Compute query, key, and value vectors.\n",
    "        q = self.get_query(x) # (n_batch, seq_len, emb_dim)\n",
    "        k = self.get_key(x)\n",
    "        v = self.get_value(x)\n",
    "\n",
    "        # Compute attention weights\n",
    "        k_transpose = k.transpose(-2, -1)\n",
    "        assert k_transpose.shape == (n_batch, emb_dim, seq_len)\n",
    "        scores = q @ k_transpose\n",
    "        assert scores.shape == (n_batch, seq_len, seq_len)\n",
    "        attention_weights = scores.softmax(dim=-1)\n",
    "\n",
    "        # Compute weighted sum of values.\n",
    "        out = attention_weights @ v\n",
    "        assert out.shape == x.shape\n",
    "        return out\n",
    "\n",
    "input_embeds = embedder(sentence_tensor)\n",
    "BareBonesSelfAttention(emb_dim)(input_embeds).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_parameters(BareBonesSelfAttention(emb_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_parameters_for_self_attention(emb_dim):\n",
    "    return (\n",
    "        # 3 linear layers (q, k, v) from emb_dim to emb_dim, each has a bias\n",
    "        ...\n",
    "        # output projection\n",
    "        + ...\n",
    "    )\n",
    "\n",
    "assert (\n",
    "    num_parameters_for_self_attention(emb_dim)\n",
    "    == num_parameters(BareBonesSelfAttention(emb_dim)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define a Transformer. Well, we'll just define a single layer. A true Transformer will have many (32, 64, 118, ...) such layers. But it's just running a bunch of copies of this layer in sequence.\n",
    "\n",
    "One new thing here is the *layer norm*. It basically rescales the activations to be mean 0 variance 1, then scales and shifts by learnable constants. So each one adds 2 * emb_dim parameters to the model, and there's one after each part (self-attention and MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 43, 5])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BareBonesTransformerLayer(nn.Module):\n",
    "    '''Implements bare-bones self-attention transformer layer, no residual connections, no dropout'''\n",
    "    def __init__(self, emb_dim, dim_feedforward):\n",
    "        super().__init__()\n",
    "        self.self_attention = BareBonesSelfAttention(emb_dim)\n",
    "        self.mlp = MLP(emb_dim, n_hidden=dim_feedforward)\n",
    "        self.norm_after_attn = nn.LayerNorm(emb_dim)\n",
    "        self.norm_after_mlp = nn.LayerNorm(emb_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.norm_after_attn(x)\n",
    "        x = self.mlp(x)\n",
    "        x = self.norm_after_mlp(x)\n",
    "        return x\n",
    "\n",
    "xformer_layer = BareBonesTransformerLayer(emb_dim, dim_feedforward=emb_dim)\n",
    "xformer_layer(input_embeds).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_parameters(xformer_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_parameters_for_transformer(emb_dim, dim_feedforward):\n",
    "    return (\n",
    "        num_parameters_for_self_attention...\n",
    "        + ...mlp...\n",
    "        # layer norms\n",
    "        + 2 * (emb_dim + emb_dim)\n",
    "    )\n",
    "assert (\n",
    "    num_parameters_for_transformer(emb_dim, dim_feedforward=emb_dim)\n",
    "    == num_parameters(xformer_layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, at long last, let's make a complete Transformer-based language model.\n",
    "\n",
    "A big new aspect here is the *position embeddings*. We'll implement *learned absolute position embeddings*, which do add parameters to the model. A cool new approach, called *Rotary Position Embeddings* ([RoPE](https://blog.eleuther.ai/rotary-embeddings/)), gets comparable or better performance without adding parameters; for an implementation of that, see [x-transformers](https://github.com/lucidrains/x-transformers/blob/75afacae30c3a6c9a9daf6f9403c3fc22f6bc945/x_transformers/x_transformers.py#L310)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1851"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, n_vocab, max_len, emb_dim, n_hidden):\n",
    "        super().__init__()\n",
    "        self.word_to_embedding = nn.Embedding(n_vocab, emb_dim)\n",
    "        self.pos_to_embedding = nn.Embedding(max_len, emb_dim)\n",
    "        self.model = BareBonesTransformerLayer(emb_dim=emb_dim, dim_feedforward=n_hidden)\n",
    "        self.lm_head = nn.Linear(emb_dim, n_vocab, bias=False)\n",
    "\n",
    "        assert self.lm_head.weight.shape == self.word_to_embedding.weight.shape\n",
    "        self.lm_head.weight = self.word_to_embedding.weight\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        input_embeds = self.word_to_embedding(input_ids)\n",
    "        # Compute position embeddings.\n",
    "        position_ids = torch.arange(input_ids.shape[-1])\n",
    "        pos_embeds = self.pos_to_embedding(position_ids)\n",
    "        x = input_embeds + pos_embeds\n",
    "        x = self.model(x)\n",
    "        return self.lm_head(x)\n",
    "\n",
    "\n",
    "xformer_lm = TransformerLM(n_vocab=n_vocab, max_len=50, emb_dim=emb_dim, n_hidden=16)\n",
    "num_parameters(xformer_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_parameters_for_transformer_lm(n_vocab, max_len, emb_dim, n_hidden):\n",
    "    return (\n",
    "        num_parameters_for_transformer(emb_dim, dim_feedforward=n_hidden)\n",
    "        + num_params_for_embedding(n_vocab=n_vocab, emb_dim=emb_dim)\n",
    "        + num_params_for_embedding(n_vocab=max_len, emb_dim=emb_dim)\n",
    "    )\n",
    "\n",
    "num_parameters_for_transformer_lm(n_vocab=n_vocab, max_len=50, emb_dim=emb_dim, n_hidden=16)\n",
    "assert (\n",
    "    num_parameters_for_transformer_lm(n_vocab=n_vocab, max_len=50, emb_dim=emb_dim, n_hidden=16)\n",
    "    == num_parameters(xformer_lm)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Q1: Why might practitioners typically tie the model weights for language modeling? Answer this by comparing the `BareBonesLM` with and without `tie_weights`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Apply what you discovered to PaLM: write an expression that shows where the number 540 billion parameters might come from in PaLM. See section 2.1 of the paper for the constants you might need. *Note*: you might not get exactly the right parameter count, but you should get in the ballpark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. How much memory would PaLM take, if each parameter is stored as a float16?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e8f4b6a4c38f2085446f7f3f791a848eb62f1444c2c1b7234c3e9244444f9b89"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
