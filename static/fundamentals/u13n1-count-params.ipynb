{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why so big? Counting parameters in sequence models\n",
    "\n",
    "PaLM has 540 billion parameters. What could they possibly all be doing? Let's figure out where the parameter budget in sequence models goes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "def num_parameters(model):\n",
    "    \"\"\"Count the number of trainable parameters in a model\"\"\"\n",
    "    return sum(param.numel() for param in model.parameters() if param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the same input setup as last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This will be the input to a language model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tensor = torch.tensor([[ord(x) for x in sentence]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = sentence_tensor[:, 1:]\n",
    "input_ids = sentence_tensor[:, :-1]\n",
    "assert input_ids.shape == targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A big chunk of the parameters of a model ends up in the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 5])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_vocab = 256\n",
    "emb_dim = 5\n",
    "embedder = nn.Embedding(n_vocab, emb_dim)\n",
    "embedder.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1280"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_parameters(embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeds = embedder(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_params_for_embedding(n_vocab, emb_dim):\n",
    "    # return ...\n",
    "    return n_vocab * emb_dim\n",
    "\n",
    "assert (\n",
    "    num_params_for_embedding(n_vocab, emb_dim)\n",
    "    == num_parameters(nn.Embedding(n_vocab, emb_dim))\n",
    ")\n",
    "assert (\n",
    "    num_params_for_embedding(50000, 2048)\n",
    "    == num_parameters(nn.Embedding(50000, 2048))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1280"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model1(nn.Module):\n",
    "    def __init__(self, n_vocab, emb_dim, tie_weights=True):\n",
    "        super().__init__()\n",
    "        self.word_to_embedding = nn.Embedding(n_vocab, emb_dim)\n",
    "        self.lm_head = nn.Linear(emb_dim, n_vocab, bias=False)\n",
    "\n",
    "        if tie_weights:\n",
    "            assert self.lm_head.weight.shape == self.word_to_embedding.weight.shape\n",
    "            self.lm_head.weight = self.word_to_embedding.weight\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        return self.lm_head(self.word_to_embedding(input_ids))\n",
    "\n",
    "\n",
    "num_parameters(Model1(n_vocab=n_vocab, emb_dim=emb_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*maybe: ask about why `bias=False`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ask about what happens when `tie_weights=False`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, emb_dim, n_hidden):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_features=emb_dim, out_features=n_hidden),\n",
    "            nn.ReLU(), # or nn.GELU() or others\n",
    "            nn.Linear(in_features=n_hidden, out_features=emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "num_parameters(MLP(emb_dim=emb_dim, n_hidden=16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget the biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num_parameters_for_mlp(emb_dim, n_hidden):\n",
    "    # return ...\n",
    "    return (emb_dim + 1) * n_hidden + (n_hidden + 1) * emb_dim\n",
    "num_parameters_for_mlp(emb_dim, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1461"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForwardLM(nn.Module):\n",
    "    def __init__(self, n_vocab, emb_dim, n_hidden, tie_weights=True):\n",
    "        super().__init__()\n",
    "        self.word_to_embedding = nn.Embedding(n_vocab, emb_dim)\n",
    "        self.model = MLP(emb_dim=emb_dim, n_hidden=n_hidden)\n",
    "        self.lm_head = nn.Linear(emb_dim, n_vocab, bias=False)\n",
    "\n",
    "        if tie_weights:\n",
    "            assert self.lm_head.weight.shape == self.word_to_embedding.weight.shape\n",
    "            self.lm_head.weight = self.word_to_embedding.weight\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        input_embeds = self.word_to_embedding(input_ids)\n",
    "        x = self.model(input_embeds)\n",
    "        return self.lm_head(x)\n",
    "\n",
    "\n",
    "ff_lm = FeedForwardLM(n_vocab=n_vocab, emb_dim=emb_dim, n_hidden=16)\n",
    "num_parameters(ff_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 42, 256])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_lm(input_ids).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "We're going to implement an oversimplified Transformer layer. If you're interested, here are a few reference implementations of the real thing:\n",
    "\n",
    "- [Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "- PyTorch's builtin implementation: [`TransformerEncoderLayer`](https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#TransformerEncoderLayer) and [`MultiheadAttention`](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention).\n",
    "- [minGPT](https://github.com/karpathy/minGPT/blob/master/mingpt/model.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 42, 5])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BareBonesSelfAttention(nn.Module):\n",
    "    '''Implements *single-head* attention, no masking, no dropout, no scaling, no init'''\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.get_query = nn.Linear(emb_dim, emb_dim)\n",
    "        self.get_key = nn.Linear(emb_dim, emb_dim)\n",
    "        self.get_value = nn.Linear(emb_dim, emb_dim)\n",
    "        self.to_output = nn.Linear(emb_dim, emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n_batch, seq_len, emb_dim = x.shape\n",
    "\n",
    "        # Compute query, key, and value vectors.\n",
    "        q = self.get_query(x) # (n_batch, seq_len, emb_dim)\n",
    "        k = self.get_key(x)\n",
    "        v = self.get_value(x)\n",
    "\n",
    "        # Compute attention weights\n",
    "        k_transpose = k.transpose(-2, -1)\n",
    "        assert k_transpose.shape == (n_batch, emb_dim, seq_len)\n",
    "        scores = q @ k_transpose\n",
    "        assert scores.shape == (n_batch, seq_len, seq_len)\n",
    "        attention_weights = scores.softmax(dim=-1)\n",
    "\n",
    "        # Compute weighted sum of values.\n",
    "        out = attention_weights @ v\n",
    "        assert out.shape == x.shape\n",
    "        return out\n",
    "\n",
    "BareBonesSelfAttention(emb_dim)(input_embeds).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 42, 5])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BareBonesTransformerLayer(nn.Module):\n",
    "    '''Implements bare-bones self-attention transformer layer, no residual connections, no dropout'''\n",
    "    def __init__(self, emb_dim, dim_feedforward):\n",
    "        super().__init__()\n",
    "        self.self_attention = BareBonesSelfAttention(emb_dim)\n",
    "        self.mlp = MLP(emb_dim, n_hidden=dim_feedforward)\n",
    "        self.norm_after_attn = nn.LayerNorm(emb_dim)\n",
    "        self.norm_after_mlp = nn.LayerNorm(emb_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.norm_after_attn(x)\n",
    "        x = self.mlp(x)\n",
    "        x = self.norm_after_mlp(x)\n",
    "        return x\n",
    "\n",
    "xformer_layer = BareBonesTransformerLayer(emb_dim, emb_dim)\n",
    "xformer_layer(input_embeds).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_parameters(xformer_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1851"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, n_vocab, max_len, emb_dim, n_hidden):\n",
    "        super().__init__()\n",
    "        self.word_to_embedding = nn.Embedding(n_vocab, emb_dim)\n",
    "        self.pos_to_embedding = nn.Embedding(max_len, emb_dim)\n",
    "        self.model = BareBonesTransformerLayer(emb_dim=emb_dim, dim_feedforward=n_hidden)\n",
    "        self.lm_head = nn.Linear(emb_dim, n_vocab, bias=False)\n",
    "\n",
    "        assert self.lm_head.weight.shape == self.word_to_embedding.weight.shape\n",
    "        self.lm_head.weight = self.word_to_embedding.weight\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        input_embeds = self.word_to_embedding(input_ids)\n",
    "        # Compute position embeddings.\n",
    "        position_ids = torch.arange(input_ids.shape[-1])\n",
    "        pos_embeds = self.pos_to_embedding(position_ids)\n",
    "        x = input_embeds + pos_embeds\n",
    "        x = self.model(x)\n",
    "        return self.lm_head(x)\n",
    "\n",
    "\n",
    "xformer_lm = TransformerLM(n_vocab=n_vocab, max_len=50, emb_dim=emb_dim, n_hidden=16)\n",
    "num_parameters(xformer_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_parameters(BareBonesSelfAttention(emb_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_parameters(nn.LayerNorm(emb_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    # Attention layer:\n",
    "    # 3 linear layers (q, k, v) from emb_dim to emb_dim, each has a bias\n",
    "    3 * emb_dim * (emb_dim + 1)\n",
    "    # output projection\n",
    "    + emb_dim * (emb_dim + 1)\n",
    "    # Feedforward:\n",
    "    # 2-layer feedforward\n",
    "    + num_parameters_for_mlp(emb_dim, emb_dim)\n",
    "    # layer norms\n",
    "    + 2 * (emb_dim + emb_dim)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_parameters(BareBonesTransformerLayer(emb_dim, emb_dim))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e8f4b6a4c38f2085446f7f3f791a848eb62f1444c2c1b7234c3e9244444f9b89"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
