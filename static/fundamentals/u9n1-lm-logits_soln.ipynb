{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "013-lm-logits",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOVes22Uvu4sPkNN1/P8/pg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kcarnold/cs344/blob/main/portfolio/fundamentals/013-lm-logits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jc8Qlh1TEgC"
      },
      "source": [
        "# `013` Language Models and Logits\n",
        "\n",
        "Task: Ask a language model for the most likely next tokens.\n",
        "\n",
        "This notebook follows up on `012-tokenization`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8_8RWp3TX-8"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUvTIxyWTdBF"
      },
      "source": [
        "We'll be using the HuggingFace Transformers library, which provides a (mostly) consistent interface to many different language models. We'll focus on the OpenAI GPT-2 model, famous for OpenAI's assertion that it was \"too dangerous\" to release in full.\n",
        "\n",
        "[Documentation](https://huggingface.co/transformers/model_doc/gpt2.html) for the model and tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWy--2nwhWPy",
        "outputId": "4d55a3c6-fb9d-4d6f-ee1e-a9e918c98e38"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.44)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osKgPaDwhaN4"
      },
      "source": [
        "import torch\n",
        "from torch import tensor"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiNKbIh8hyDg"
      },
      "source": [
        "### Download and load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM5o_4w1hfyV"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\", add_prefix_space=True) # smaller version of GPT-2\n",
        "# Alternative to add_prefix_space is to use `is_split_into_words=True`\n",
        "# add the EOS token as PAD token to avoid warnings\n",
        "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\", pad_token_id=tokenizer.eos_token_id)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-Z9_U0LUEVQ",
        "outputId": "cb7d4eb7-bc54-4583-dd10-e0cb185494da"
      },
      "source": [
        "print(f\"The model has {model.num_parameters():,d} parameters.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 81,912,576 parameters.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOUiz_PsUZgS"
      },
      "source": [
        "## Task\n",
        "\n",
        "Consider the following phrase: \"This weekend I plan to\".\n",
        "\n",
        "1. Convert the phrase into token ids.\n",
        "2. Use the `forward` method of the `model`. Explain the shape of `model_output.logits`.\n",
        "3. Pull out the logits corresponding to the *last* token in the input phrase. Identify the id of the most likely next token.\n",
        "4. Find what token the model thinks is the most likely.\n",
        "5. Use the `topk` method to find the top-20 most likely choices for the next token. \n",
        "6. Write a function that is given a phrase and a *k* and returns the top *k* most likely next tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JS7Z-DjoUiLK"
      },
      "source": [
        "#phrase = \"In a shocking finding, scientists discovered a herd of unicorns living in\"\n",
        "phrase = \"This weekend I plan to\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpyeKakrjfpt"
      },
      "source": [
        "# input_ids = ...\n",
        "input_ids = tokenizer.encode(phrase)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEy1QBTDotjU",
        "outputId": "70a3312b-1bc7-47b0-c1d6-4739c93f56c0"
      },
      "source": [
        "model_output = model.forward(tensor([input_ids]))\n",
        "model_output.logits.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 50257])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EHMGJFPS05B"
      },
      "source": [
        "# since we only have a single sequence (batch size of 1), let's collapse the batch dimension.\n",
        "logits = model_output.logits[0]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJWXqQLLkCyP",
        "outputId": "b969f2ed-2289-48c2-e717-b4802a493c5d"
      },
      "source": [
        "# your code here\n",
        "logits[-1].argmax()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(467)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "xhL9qP4HpJlx",
        "outputId": "8a014913-7577-4b14-e987-c0c126994df5"
      },
      "source": [
        "# your code here\n",
        "tokenizer.decode([467])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' go'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsI_Tz0ipglx"
      },
      "source": [
        "## Analysis\n",
        "\n",
        "What would be required to generate more than one token? What decisions would you have to make?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNL1iHs_phOB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}