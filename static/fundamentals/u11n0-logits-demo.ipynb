{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fma8lEnQCwia",
        "outputId": "d6ea3fb5-56a8-4cbb-ddbc-399b5b86c67f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "# If the import fails, uncomment the following line:\n",
        "!pip install transformers\n",
        "import torch\n",
        "from torch import tensor\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import pandas as pd\n",
        "# Avoid a warning message\n",
        "import os; os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\", add_prefix_space=True) # smaller version of GPT-2\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = 'left'\n",
        "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")"
      ],
      "metadata": {
        "id": "zS7NkVLSCxSU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The tokenizer has {len(tokenizer.get_vocab())} strings in its vocabulary.\")\n",
        "print(f\"The model has {model.num_parameters():,d} parameters.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjVGKgReEcrZ",
        "outputId": "7f7e414c-7af4-4128-9115-9e762f341173"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tokenizer has 50257 strings in its vocabulary.\n",
            "The model has 81,912,576 parameters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding"
      ],
      "metadata": {
        "id": "P3vt8iYJDfux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model can be trained and evaluated with several independent sequences at a time. It wasn't at training time, so we had to set a few flags above, but now this will work:"
      ],
      "metadata": {
        "id": "NwBttj9nDxJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phrase = \"This weekend I plan to\""
      ],
      "metadata": {
        "id": "vqyYB1ZOFICo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = tokenizer([\"Hi\", phrase], padding=True, return_tensors='pt')\n",
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIurnbtODPDw",
        "outputId": "43b036ba-6f54-48c9-85ac-5e78d72abab5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[50256, 50256, 50256, 50256, 15902],\n",
              "        [  770,  5041,   314,  1410,   284]]), 'attention_mask': tensor([[0, 0, 0, 0, 1],\n",
              "        [1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch['input_ids'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBgSLxKbD_6O",
        "outputId": "a4399719-cc0f-45fe-9b87-aae3d0833e8b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that `input_ids` is 2 (number of sequences in the batch) by 5 (number of tokens in the longest sequence.\n",
        "\n",
        "The `attention_mask` is used by the model to make sure that the padding tokens aren't used in any of the model's calculations. We won't be needing it in these demos, but generally it is passed in."
      ],
      "metadata": {
        "id": "zvvTffCYDJ13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Going forward we'll use this simple example:"
      ],
      "metadata": {
        "id": "i2MP8eqcFYkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(phrase, return_tensors='pt')['input_ids']; input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cm_S3WezFauO",
        "outputId": "dcb3cfff-8522-4ef0-fd3e-7ccff13de6a5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 770, 5041,  314, 1410,  284]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings"
      ],
      "metadata": {
        "id": "HlS7lKlCEQE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model includes two modules that are very important: one at the very beginning, one at the very end."
      ],
      "metadata": {
        "id": "t-7UhZNDEVHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_embedding_module = model.transformer.wte\n",
        "token_embedding_module"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge_nh8t5EggA",
        "outputId": "78efbfe9-4b2e-4e29-f87e-be0b832ab722"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50257, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm_head_module = model.lm_head\n",
        "lm_head_module"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5nfh8lHEjRm",
        "outputId": "8722d42a-009d-4535-d77a-be04de3dc6c2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=768, out_features=50257, bias=False)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the dimensionality is exactly symmetrical: `token_embedding` takes each token id and maps it to one of the 50k possible token embeddings (each one 768-dimensional); `lm_head` takes embeddings and maps them to logits corresponding to each of the 50k vocab entries."
      ],
      "metadata": {
        "id": "IipzRn_LEorn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It turns out that for this model, the token embeddings are identical on the input and output. This is called \"tied weights\" and is quite common now, to save parameters. This is easy to see and implement in PyTorch because `Linear` layers store their `W` matrices transposed internally already."
      ],
      "metadata": {
        "id": "OLu8QFx5GmPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(token_embedding_module.weight.data == lm_head_module.weight.data).all()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-maXEohGqbD",
        "outputId": "57ae4178-ebf8-442b-da31-0f6eed41ffd2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example of mapping"
      ],
      "metadata": {
        "id": "O--n7iKNFQZt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last token id is:"
      ],
      "metadata": {
        "id": "8vBnwBU5FSvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_ids[0, -1],\n",
        "    \"which corresponds to\",\n",
        "    repr(tokenizer.decode(input_ids[0, -1])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9j4cHr8gFUdo",
        "outputId": "2adcbe60-62a3-441a-b4d4-bef0efe58683"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(284) which corresponds to ' to'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It has vector:"
      ],
      "metadata": {
        "id": "PZhfvNtCFj5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    vec = token_embedding_module(input_ids[0, -1])\n",
        "vec.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQ7GwOI9GOEN",
        "outputId": "579fae93-4f99-4e9f-f774-2fa40b588fb2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([768])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(The specific numbers in there are illegibile, so we hide them.)"
      ],
      "metadata": {
        "id": "9WDXEi6_GV6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passing a vector through a linear layer is equivalent to computing the dot product with all of its rows, so we're going to see the dot product of `vec` with all of the token embeddings."
      ],
      "metadata": {
        "id": "osay9lXBGZit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    logits = lm_head_module(vec)\n",
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16IE274rGlFj",
        "outputId": "02557fed-092d-4bde-d583-38cb5608987f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50257])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[tokenizer.decode(x) for x in logits.topk(k=10).indices]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_QmCn68HMKS",
        "outputId": "89ad15cc-d164-4609-840b-936cd93ffc75"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' to', 'to', ' To', 'To', ' for', ' in', ' with', ' on', ' TO', ' and']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Astute observers will notice that the token space is wasted by those minor variants of the same token. Current research has improved on this slightly by allowing these related tokens to share information, but it doesn't make a big difference.\n"
      ],
      "metadata": {
        "id": "cVpOMih4HVro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we do this for all the input tokens at the same time, we get the most similar tokens for each input token. That will almost always be the token itself, but note that the token embeddings are not explicitly normalized so the dot product with a different token's embedding may turn out to be the largest one just because it's a different magnitude."
      ],
      "metadata": {
        "id": "IYSNXSOzHfqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "logits = lm_head_module(token_embedding_module(input_ids))\n",
        "pd.DataFrame([\n",
        "    [tokenizer.decode(x) for x in y]\n",
        "    for y in logits.topk(k=10).indices[0]\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "98F0s5BjHm6R",
        "outputId": "03b2dd36-9f31-4591-93c3-340efb604fcb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1          2      3           4         5      6        7  \\\n",
              "0      This      This      These   this        THIS        It   this    These   \n",
              "1   weekend   Weekend   weekends   week   afternoon   evening   week   Sunday   \n",
              "2         I         I         we     my          We      they     me       My   \n",
              "3      plan     plans       plan   Plan       Plans      Plan   PLAN   intend   \n",
              "4        to        to         To     To         for        in   with       on   \n",
              "\n",
              "           8         9  \n",
              "0        The      That  \n",
              "1     Friday  Saturday  \n",
              "2         My       you  \n",
              "3   planning   planned  \n",
              "4         TO       and  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-95a27300-7eeb-4b5b-94f2-ef16461aafa8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This</td>\n",
              "      <td>This</td>\n",
              "      <td>These</td>\n",
              "      <td>this</td>\n",
              "      <td>THIS</td>\n",
              "      <td>It</td>\n",
              "      <td>this</td>\n",
              "      <td>These</td>\n",
              "      <td>The</td>\n",
              "      <td>That</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>weekend</td>\n",
              "      <td>Weekend</td>\n",
              "      <td>weekends</td>\n",
              "      <td>week</td>\n",
              "      <td>afternoon</td>\n",
              "      <td>evening</td>\n",
              "      <td>week</td>\n",
              "      <td>Sunday</td>\n",
              "      <td>Friday</td>\n",
              "      <td>Saturday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I</td>\n",
              "      <td>I</td>\n",
              "      <td>we</td>\n",
              "      <td>my</td>\n",
              "      <td>We</td>\n",
              "      <td>they</td>\n",
              "      <td>me</td>\n",
              "      <td>My</td>\n",
              "      <td>My</td>\n",
              "      <td>you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>plan</td>\n",
              "      <td>plans</td>\n",
              "      <td>plan</td>\n",
              "      <td>Plan</td>\n",
              "      <td>Plans</td>\n",
              "      <td>Plan</td>\n",
              "      <td>PLAN</td>\n",
              "      <td>intend</td>\n",
              "      <td>planning</td>\n",
              "      <td>planned</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>to</td>\n",
              "      <td>to</td>\n",
              "      <td>To</td>\n",
              "      <td>To</td>\n",
              "      <td>for</td>\n",
              "      <td>in</td>\n",
              "      <td>with</td>\n",
              "      <td>on</td>\n",
              "      <td>TO</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95a27300-7eeb-4b5b-94f2-ef16461aafa8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-95a27300-7eeb-4b5b-94f2-ef16461aafa8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-95a27300-7eeb-4b5b-94f2-ef16461aafa8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What the model does"
      ],
      "metadata": {
        "id": "jt1d8gr3Id4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the model processes its input, it first looks up the embedding for each input token to produce its first \"hidden states\". Then it incrementally applies each layer of the model (consisting, in this case, of a self-attention \"mixing\" layer followed by a (one-token-at-a-time) feed-forward \"mapping\" layer), obtaining incrementally more refined hidden states that approach the context vector for the next token."
      ],
      "metadata": {
        "id": "a_Kk7xInIg7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    model_output = model(input_ids, output_hidden_states=True)\n",
        "hidden_states = model_output.hidden_states"
      ],
      "metadata": {
        "id": "2O6A4AEdIiT_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(hidden_states) # this is model.config.n_layer + 1, to include the input embeddings."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCK1A85xIoJf",
        "outputId": "8059b757-758d-4ead-e2cf-e8adf3ab9370"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = lm_head_module(hidden_states[0])\n",
        "pd.DataFrame([\n",
        "    [tokenizer.decode(x) for x in y]\n",
        "    for y in logits.topk(k=10).indices[0]\n",
        "]).T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "HRVNcX7qJKe1",
        "outputId": "eb9037e4-4978-4700-d02d-2c5006615b56"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               0           1        2          3     4\n",
              "0           This     weekend        I       plan    to\n",
              "1           This    weekends        I      plans    to\n",
              "2          These     Weekend       we       plan    To\n",
              "3  <|endoftext|>        week       my       Plan   for\n",
              "4        theless   afternoon       me      Plans    To\n",
              "5             It     evening       We       Plan    in\n",
              "6           THIS        week      you       PLAN    on\n",
              "7          There   fortnight   myself     intend    TO\n",
              "8           this    holidays       My   planning   and\n",
              "9            You   Saturdays      You   proposal    of"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aeee0fa1-57e7-42ae-a5f1-d186cf7bf923\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This</td>\n",
              "      <td>weekend</td>\n",
              "      <td>I</td>\n",
              "      <td>plan</td>\n",
              "      <td>to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This</td>\n",
              "      <td>weekends</td>\n",
              "      <td>I</td>\n",
              "      <td>plans</td>\n",
              "      <td>to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>These</td>\n",
              "      <td>Weekend</td>\n",
              "      <td>we</td>\n",
              "      <td>plan</td>\n",
              "      <td>To</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;|endoftext|&gt;</td>\n",
              "      <td>week</td>\n",
              "      <td>my</td>\n",
              "      <td>Plan</td>\n",
              "      <td>for</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>theless</td>\n",
              "      <td>afternoon</td>\n",
              "      <td>me</td>\n",
              "      <td>Plans</td>\n",
              "      <td>To</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>It</td>\n",
              "      <td>evening</td>\n",
              "      <td>We</td>\n",
              "      <td>Plan</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>THIS</td>\n",
              "      <td>week</td>\n",
              "      <td>you</td>\n",
              "      <td>PLAN</td>\n",
              "      <td>on</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>There</td>\n",
              "      <td>fortnight</td>\n",
              "      <td>myself</td>\n",
              "      <td>intend</td>\n",
              "      <td>TO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>this</td>\n",
              "      <td>holidays</td>\n",
              "      <td>My</td>\n",
              "      <td>planning</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>You</td>\n",
              "      <td>Saturdays</td>\n",
              "      <td>You</td>\n",
              "      <td>proposal</td>\n",
              "      <td>of</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aeee0fa1-57e7-42ae-a5f1-d186cf7bf923')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aeee0fa1-57e7-42ae-a5f1-d186cf7bf923 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aeee0fa1-57e7-42ae-a5f1-d186cf7bf923');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = lm_head_module(hidden_states[-1])\n",
        "pd.DataFrame([\n",
        "    [tokenizer.decode(x) for x in y]\n",
        "    for y in logits.topk(k=10).indices[0]\n",
        "]).T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "erBloDG7Jor1",
        "outputId": "3e10567f-954e-4665-bcc4-872b75b992cb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       0     1          2      3        4\n",
              "0    The     ,        was     to       go\n",
              "1      A    in        had     on     take\n",
              "2      .   was        got      a    spend\n",
              "3     \\n    's       went    for     make\n",
              "4    The    at    decided     my       do\n",
              "5      ,    is       took    not       be\n",
              "6   This     I   received     an   attend\n",
              "7      I     �    started    the    visit\n",
              "8    the    we      spent    and      run\n",
              "9     It   the        met   this     have"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7286aa25-2bd1-4986-9b20-8aa831cbafec\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The</td>\n",
              "      <td>,</td>\n",
              "      <td>was</td>\n",
              "      <td>to</td>\n",
              "      <td>go</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A</td>\n",
              "      <td>in</td>\n",
              "      <td>had</td>\n",
              "      <td>on</td>\n",
              "      <td>take</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>.</td>\n",
              "      <td>was</td>\n",
              "      <td>got</td>\n",
              "      <td>a</td>\n",
              "      <td>spend</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\n</td>\n",
              "      <td>'s</td>\n",
              "      <td>went</td>\n",
              "      <td>for</td>\n",
              "      <td>make</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The</td>\n",
              "      <td>at</td>\n",
              "      <td>decided</td>\n",
              "      <td>my</td>\n",
              "      <td>do</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>,</td>\n",
              "      <td>is</td>\n",
              "      <td>took</td>\n",
              "      <td>not</td>\n",
              "      <td>be</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>This</td>\n",
              "      <td>I</td>\n",
              "      <td>received</td>\n",
              "      <td>an</td>\n",
              "      <td>attend</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>I</td>\n",
              "      <td>�</td>\n",
              "      <td>started</td>\n",
              "      <td>the</td>\n",
              "      <td>visit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>the</td>\n",
              "      <td>we</td>\n",
              "      <td>spent</td>\n",
              "      <td>and</td>\n",
              "      <td>run</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>It</td>\n",
              "      <td>the</td>\n",
              "      <td>met</td>\n",
              "      <td>this</td>\n",
              "      <td>have</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7286aa25-2bd1-4986-9b20-8aa831cbafec')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7286aa25-2bd1-4986-9b20-8aa831cbafec button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7286aa25-2bd1-4986-9b20-8aa831cbafec');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: the logits after the first token seem messed up. I suspect an issue with the \"distilling\" part of this model's training. All of the other token distributions seem to be good."
      ],
      "metadata": {
        "id": "1umj-doBJ-FV"
      }
    }
  ]
}