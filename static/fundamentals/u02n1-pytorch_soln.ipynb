{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f5fe490",
   "metadata": {},
   "source": [
    "## PyTorch Warmup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PyTorch](https://pytorch.org/) is the open-source machine learning framework that we'll be using in this class. It has a wide range of functionality; for now we'll just get started with some of its very basic array-processing functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot Products\n",
    "\n",
    "The most common basic primitive in a neural network is a *linear* layer (you'll sometimes see it called a \"Dense\" layer). These are where almost all of the parameters go in a network. (Some architectures use a variant called a *convolutional* layer.) At its core, a linear layer does a bunch of *dot product*s between its *input* vector and its (learned) *weight* vectors.\n",
    "\n",
    "A few intuitions to understand what a dot product is:\n",
    "\n",
    "1. It measures *similarity*, in the sense of *alignment*. The following statements loosely capture it:\n",
    "  - \"How much does the input look like *this*?\"\n",
    "  - \"How big is the input in *this* direction?\"\n",
    "  - \"How aligned is the input with this direction?\"\n",
    "  - \"What's the cosine of the angle between the input vector and this vector?\"\n",
    "2. A bunch of dot products all together (like in a Linear layer) *rotates and stretches* the input space, like moving a camera around a scene.\n",
    "3. It's how a multiple linear regression computes its output: a weighted mixture of each part of its input.\n",
    "\n",
    "Recall that we can make a line by an expression like `y = w*x + b`. (Some of you may remember *mx+b* , but we'll use *w* for the *weight(s)* instead.)\n",
    "\n",
    "That's a multiplication followed by a sum. We can extend that to lots of *x*'s, each of which needs a corresponding *w*:\n",
    "\n",
    "`y = w1*x1 + w2*x2 + ... + wN*xN + b`\n",
    "\n",
    "For simplicity, let's start by ignoring the `b`ias.  So we're left with\n",
    "\n",
    "`y = w1*x1 + w2*x2 + ... + wN*xN`\n",
    "\n",
    "that is, multiply each number in `w` by its corresponding number in `x` and add up the results: `sum(w[i] * x[i] for i in range(N))`.\n",
    "\n",
    "The result is called a *dot product*, and is one of the fundamental operations in linear algebra. At this point you don't need to understand all the linear algebra part of this, we're just implementing a common calculation.\n",
    "\n",
    "Let's do that in pure Python, and then in PyTorch. To start, let's make a `w`eights and an `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d290d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2., -1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor([2.0, -1.0])\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b9487b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5000, -3.0000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.5, -3.0])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbafdfed",
   "metadata": {},
   "source": [
    "The shapes of `w` and `x` must match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cbce88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(w)\n",
    "assert N == len(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b38d0106",
   "metadata": {},
   "source": [
    "#### `for` loop approach\n",
    "\n",
    "**Task**: *Write a function that uses a `for` loop* to compute the dot product of `w` and `x`. Name the function `dot_loop`. Check that you get `6.0` for the `w` and `x` provided in the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6255210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dot_loop(w, x):\n",
    "    # return 0.0 # FIXME <<END\n",
    "    N = len(w)\n",
    "    return sum(w[i] * x[i] for i in range(N))\n",
    "    # END\n",
    "dot_loop(w, x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some test cases that `dot_loop` should pass. You don't need to understand how this code works yet, but it would reward some study. (Note that, like most tests, if it passes you'll see no output when the cell runs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    ([0.], [500.], 0.0),\n",
    "    ([1., 0.0], [50.0, .5], 50.0),\n",
    "    ([-1.0, 1.0], [-1.0, 1.0], 2.0)\n",
    "]\n",
    "def run_dot_tests(f):\n",
    "    assert all(\n",
    "        torch.isclose(\n",
    "            f(torch.tensor(w), torch.tensor(x)),\n",
    "            torch.tensor(prod))\n",
    "        for w, x, prod in test_cases)\n",
    "run_dot_tests(dot_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3414e8d",
   "metadata": {},
   "source": [
    "#### Torch Elementwise Operations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Torch Elementwise Operations\n",
    "\n",
    "But that's a lot of typing for a concept that we're going to use very frequently. To shorten it (and make it run way faster too!), we'll start taking advantage of some of Torch's builtin functionality.\n",
    "\n",
    "First, we'll learn about *elementwise operations* (called *pointwise operations* in the [PyTorch docs](https://pytorch.org/docs/stable/torch.html#pointwise-ops)).\n",
    "\n",
    "If you try to `*` two Python lists together, you get a `TypeError` (how do you multiply lists??). But in PyTorch (and NumPy, which it's heavily based on), array operations happen *element-by-element* (sometimes called *elementwise*): to multiply two tensors that have the same shape, multiply each number in the first tensor with the corresponding number of the second tensor. The result is a new tensor of the same shape with all the elementwise products.\n",
    "\n",
    "{{% task %}}Try running `w * x`{{% /task %}}\n",
    "\n",
    "Torch also provides [*reduction* methods](https://pytorch.org/docs/stable/torch.html#reduction-ops), so named because they *reduce* the number of elements in a Tensor.\n",
    "\n",
    "One really useful reduction op is `.sum`.\n",
    "\n",
    "{{% task %}}Try running `w.sum()`.{{% /task %}}\n",
    "\n",
    "> You can also write that as `torch.sum(w)`.\n",
    "\n",
    "{{% task %}}Now **make a new version of `dot_loop`, called `dot_ops`**, that uses an elementwise op to multiply corresponding numbers and a reduction op to sum the result. Check that the result is the same.{{% /task %}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c901a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_ops(w, x):\n",
    "    return 0.0\n",
    "dot_ops(w, x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Torch Elementwise Operations\n",
    "\n",
    "But that's a lot of typing for a concept that we're going to use very frequently. To shorten it (and make it run way faster too!), we'll start taking advantage of some of Torch's builtin functionality.\n",
    "\n",
    "First, we'll learn about *elementwise operations* (called *pointwise operations* in the [PyTorch docs](https://pytorch.org/docs/stable/torch.html#pointwise-ops)).\n",
    "\n",
    "If you try to `*` two Python lists together, you get a `TypeError` (how do you multiply lists??). But in PyTorch (and NumPy, which it's heavily based on), array operations happen *element-by-element* (sometimes called *elementwise*): to multiply two tensors that have the same shape, multiply each number in the first tensor with the corresponding number of the second tensor. The result is a new tensor of the same shape with all the elementwise products.\n",
    "\n",
    "**Task**: Predict what you'll get from running `w * x`. Then try it and compare with your prediction. (No answer needed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Reduction Ops\n",
    "\n",
    "Torch also provides [*reduction* methods](https://pytorch.org/docs/stable/torch.html#reduction-ops), so named because they *reduce* the number of elements in a Tensor.\n",
    "\n",
    "One really useful reduction op is `.sum`. (I also frequently use `.mean`, `.max`, and `.argmax`).\n",
    "\n",
    "**Task**: Predict the output of running `x.sum()` Then try it and compare with your prediction.\n",
    "\n",
    "> You can also write that as `torch.sum(w)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a dot product out of Torch ops\n",
    "\n",
    "Now **make a new version of `dot_loop`, called `dot_ops`**, that uses an elementwise op to multiply corresponding numbers and a reduction op to sum the result. Check that the result still passes the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, since `dot` is such an important operation, PyTorch provides it directly:\n",
    "\n",
    "```python\n",
    "torch.dot(w, x)\n",
    "```\n",
    "\n",
    "Python recently introduced a \"matmul operator\", `@`, that does the same thing.\n",
    "\n",
    "```python\n",
    "w @ x\n",
    "```\n",
    "\n",
    "To apply this knowledge, let's try writing a slightly more complex function: a linear transformation layer.\n",
    "\n",
    "## Linear Layer\n",
    "\n",
    "The most basic component of a neural network (and many other machine learning methods) is a *linear transformation layer*. Going back to our `y = w*x + b` example, the `w*x + b` is the linear transformation: given an `x`, dot it with some `w`eights and add a `b`ias.\n",
    "\n",
    "{{% task %}}\n",
    "**Write a function that performs a linear transformation of a vector `x`.** Use PyTorch's built-in functionality for dot products.\n",
    "{{% /task %}}\n",
    "\n",
    "### Linear layer, Module-style\n",
    "\n",
    "Notice that `linear`'s job is to transform `x`, but it needed 3 parameters, not just 1. It would be convenient to view the `linear` function as simply a function of `x`, with `weights` and `bias` being internal details.\n",
    "\n",
    "One way to do this is to make a `Linear` class that has these as parameters. Fill in the blanks in the template code to do this.\n",
    "\n",
    "## Mean Squared Error\n",
    "\n",
    "Now let's apply what you just learned about elementwise operations on PyTorch tensors to another very common building block in machine learning: measuring *error*.\n",
    "\n",
    "Once we make some predictions, we usually want to be able to measure how *good* the predictions were. For regression tasks, i.e., tasks where we're predicting *numbers*, one very common measure is the *mean squared error*. Here's an algorithm to compute it:\n",
    "\n",
    "- compute `resid` as true (`y_true`) minus predicted (`y_pred`).\n",
    "- compute `squared_error` by squaring each number in `resid`\n",
    "- compute `mean_squared_error` by taking the `mean` of `squared_error`.\n",
    "\n",
    "> **Technical note**: This process implements the mean squared error *loss function*. That is a function that is given some *true* values (call them `$y_1$` through `$y_n$`) and some *predicted* values (call them `$\\hat{y}_1$` through `$\\hat{y}_n$`) and returns `$$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2.$$`\n",
    "\n",
    "Generally you'd get the predicted values, `y_pred`, by calling a function that implements a model (like `linear.forward()` above. But to focus our attention on the error computation, we've provided sample values for `y_true` and `y_pred` in the template that you can just use as-is.\n",
    "\n",
    "{{% task %}}\n",
    "\n",
    "1. Implement each line of the above algorithm in PyTorch code.\n",
    "    - Use separate cells so you can check the results along the way. For example, the first cell should have two lines, the first to assign (`resid = ...`) and the second to show the result (`resid`).\n",
    "    - **You should not need to write any loops.**\n",
    "    - Try using both `squared_error.mean()` and `torch.mean(squared_error)`.\n",
    "2. Now, write the entire computation in a single succinct expression (i.e., without having to create intermediate variables for `resid` and `squared_error`). Check that you get the same result.\n",
    "\n",
    "{{% /task %}}\n",
    "\n",
    "Notes:\n",
    "\n",
    "- Recall that Python's exponentiation operator is `**`.\n",
    "- PyTorch tensors also have a `.pow()` method. So you might see `.pow(2)`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed68b87",
   "metadata": {},
   "source": [
    "### Dot products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49040893",
   "metadata": {},
   "source": [
    "## Linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f44bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(weights, bias, x):\n",
    "    return 0.0 # FIXME\n",
    "linear(w, 1.0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe09e53f",
   "metadata": {},
   "source": [
    "### Linear layer, Module-style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b739211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = ...\n",
    "        self.bias = ...\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return ...\n",
    "\n",
    "layer = Linear(weights=w, bias=1.0)\n",
    "layer.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3a1db8",
   "metadata": {},
   "source": [
    "## Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca6f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tensor([3.14, 1.59, 2.65])\n",
    "y_pred = tensor([2.71, 8.28, 1.83])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fa22",
   "language": "python",
   "name": "py3-fa22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74acacffd0749514a06dc7848c6df011b7a94ca944d93916dccfefa4cad34482"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
