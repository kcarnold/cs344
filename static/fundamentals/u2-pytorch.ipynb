{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f5fe490",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed68b87",
   "metadata": {},
   "source": [
    "### Dot products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260bba59",
   "metadata": {},
   "source": [
    "Recall that we can make a line by an expression like `y = w*x + b`. (Some of you may remember *mx+b* , but we'll use *w* for the *weight(s)* instead.)\n",
    "\n",
    "That's a multiplication followed by a sum. We can extend that to lots of *x*'s, each of which needs a corresponding *w*:\n",
    "\n",
    "`y = w1*x1 + w2*x2 + ... + wN*xN + b`\n",
    "\n",
    "For simplicity, let's start by ignoring the `b`ias.  So we're left with\n",
    "\n",
    "`y = w1*x1 + w2*x2 + ... + wN*xN`\n",
    "\n",
    "that is, multiply each number in `w` by its corresponding number in `x` and add up the results: `sum(w[i] * x[i] for i in range(N))`.\n",
    "\n",
    "The result is called a *dot product*, and is one of the fundamental operations in linear algebra. At this point you don't need to understand all the linear algebra part of this, we're just implementing a common calculation.\n",
    "\n",
    "Let's do that in Python, and then Torch. To start, let's make a `w`eights and an `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d290d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tensor([-2.0, 1.0])\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9487b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tensor([1.5, 2.0])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbafdfed",
   "metadata": {},
   "source": [
    "The shapes of `w` and `x` must match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbce88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(w)\n",
    "assert N == len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38d0106",
   "metadata": {},
   "source": [
    "#### `for` loop approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6255210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_loop(w, x):\n",
    "    return 0.0 # FIXME\n",
    "dot_loop(w, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3414e8d",
   "metadata": {},
   "source": [
    "#### Torch Elementwise Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c901a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_ops(w, x):\n",
    "    return 0.0\n",
    "dot_ops(w, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49040893",
   "metadata": {},
   "source": [
    "## Linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f44bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(weights, bias, x):\n",
    "    return 0.0 # FIXME\n",
    "linear(w, 1.0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe09e53f",
   "metadata": {},
   "source": [
    "### Linear layer, Module-style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b739211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = ...\n",
    "        self.bias = ...\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return ...\n",
    "\n",
    "layer = Linear(weights=w, bias=1.0)\n",
    "layer.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3a1db8",
   "metadata": {},
   "source": [
    "## Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca6f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tensor([3.14, 1.59, 2.65])\n",
    "y_pred = tensor([2.71, 8.28, 1.83])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
