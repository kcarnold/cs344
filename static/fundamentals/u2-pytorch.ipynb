{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f5fe490",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "\n",
    "## PyTorch\n",
    "\n",
    "[PyTorch](https://pytorch.org/) is the open-source machine learning framework that we'll be using in this class. It has a wide range of functionality, but today we'll just get started with some of its very basic array-processing functionality.\n",
    "\n",
    "### Dot Products\n",
    "\n",
    "Recall that we can make a line by an expression like `y = w*x + b`. (Some of you may remember *mx+b* , but we'll use *w* for the *weight(s)* instead.)\n",
    "\n",
    "That's a multiplication followed by a sum. We can extend that to lots of *x*'s, each of which needs a corresponding *w*:\n",
    "\n",
    "`y = w1*x1 + w2*x2 + ... + wN*xN + b`\n",
    "\n",
    "For simplicity, let's start by ignoring the `b`ias.  So we're left with\n",
    "\n",
    "`y = w1*x1 + w2*x2 + ... + wN*xN`\n",
    "\n",
    "that is, multiply each number in `w` by its corresponding number in `x` and add up the results: `sum(w[i] * x[i] for i in range(N))`.\n",
    "\n",
    "The result is called a *dot product*, and is one of the fundamental operations in linear algebra. At this point you don't need to understand all the linear algebra part of this, we're just implementing a common calculation.\n",
    "\n",
    "Let's do that in Python, and then Torch. To start, let's make a `w`eights and an `x`. (See the notebook.) Note that the shapes must match.\n",
    "\n",
    "#### `for` loop approach\n",
    "\n",
    "{{% task %}}\n",
    "**Write a function that uses a `for` loop** to compute the dot product of `w` and `x`. Name the function `dot_loop`. Check that you get `-1.0` for the `w` and `x` provided in the template.\n",
    "{{% /task %}}\n",
    "\n",
    "#### Torch Elementwise Operations\n",
    "\n",
    "But that's a lot of typing for a concept that we're going to use very frequently. To shorten it (and make it run way faster too!), we'll start taking advantage of some of Torch's builtin functionality.\n",
    "\n",
    "First, we'll learn about *elementwise operations* (called *pointwise operations* in the [PyTorch docs](https://pytorch.org/docs/stable/torch.html#pointwise-ops)).\n",
    "\n",
    "If you try to `*` two Python lists together, you get a `TypeError` (how do you multiply lists??). But in PyTorch (and NumPy, which it's heavily based on), array operations happen *element-by-element* (sometimes called *elementwise*): to multiply two tensors that have the same shape, multiply each number in the first tensor with the corresponding number of the second tensor. The result is a new tensor of the same shape with all the elementwise products.\n",
    "\n",
    "{{% task %}}Try running `w * x`{{% /task %}}\n",
    "\n",
    "Torch also provides [*reduction* methods](https://pytorch.org/docs/stable/torch.html#reduction-ops), so named because they *reduce* the number of elements in a Tensor.\n",
    "\n",
    "One really useful reduction op is `.sum`.\n",
    "\n",
    "{{% task %}}Try running `w.sum()`.{{% /task %}}\n",
    "\n",
    "> You can also write that as `torch.sum(w)`.\n",
    "\n",
    "{{% task %}}Now **make a new version of `dot_loop`, called `dot_ops`**, that uses an elementwise op to multiply corresponding numbers and a reduction op to sum the result. Check that the result is the same.{{% /task %}}\n",
    "\n",
    "Finally, since `dot` is such an important operation, PyTorch provides it directly:\n",
    "\n",
    "```python\n",
    "torch.dot(w, x)\n",
    "```\n",
    "\n",
    "Python recently introduced a \"matmul operator\", `@`, that does the same thing.\n",
    "\n",
    "```python\n",
    "w @ x\n",
    "```\n",
    "\n",
    "To apply this knowledge, let's try writing a slightly more complex function: a linear transformation layer.\n",
    "\n",
    "## Linear Layer\n",
    "\n",
    "The most basic component of a neural network (and many other machine learning methods) is a *linear transformation layer*. Going back to our `y = w*x + b` example, the `w*x + b` is the linear transformation: given an `x`, dot it with some `w`eights and add a `b`ias.\n",
    "\n",
    "{{% task %}}\n",
    "**Write a function that performs a linear transformation of a vector `x`.** Use PyTorch's built-in functionality for dot products.\n",
    "{{% /task %}}\n",
    "\n",
    "### Linear layer, Module-style\n",
    "\n",
    "Notice that `linear`'s job is to transform `x`, but it needed 3 parameters, not just 1. It would be convenient to view the `linear` function as simply a function of `x`, with `weights` and `bias` being internal details.\n",
    "\n",
    "One way to do this is to make a `Linear` class that has these as parameters. Fill in the blanks in the template code to do this.\n",
    "\n",
    "## Mean Squared Error\n",
    "\n",
    "Now let's apply what you just learned about elementwise operations on PyTorch tensors to another very common building block in machine learning: measuring *error*.\n",
    "\n",
    "Once we make some predictions, we usually want to be able to measure how *good* the predictions were. For regression tasks, i.e., tasks where we're predicting *numbers*, one very common measure is the *mean squared error*. Here's an algorithm to compute it:\n",
    "\n",
    "- compute `resid` as true (`y_true`) minus predicted (`y_pred`).\n",
    "- compute `squared_error` by squaring each number in `resid`\n",
    "- compute `mean_squared_error` by taking the `mean` of `squared_error`.\n",
    "\n",
    "> **Technical note**: This process implements the mean squared error *loss function*. That is a function that is given some *true* values (call them `$y_1$` through `$y_n$`) and some *predicted* values (call them `$\\hat{y}_1$` through `$\\hat{y}_n$`) and returns `$$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2.$$`\n",
    "\n",
    "Generally you'd get the predicted values, `y_pred`, by calling a function that implements a model (like `linear.forward()` above. But to focus our attention on the error computation, we've provided sample values for `y_true` and `y_pred` in the template that you can just use as-is.\n",
    "\n",
    "{{% task %}}\n",
    "\n",
    "1. Implement each line of the above algorithm in PyTorch code.\n",
    "    - Use separate cells so you can check the results along the way. For example, the first cell should have two lines, the first to assign (`resid = ...`) and the second to show the result (`resid`).\n",
    "    - **You should not need to write any loops.**\n",
    "    - Try using both `squared_error.mean()` and `torch.mean(squared_error)`.\n",
    "2. Now, write the entire computation in a single succinct expression (i.e., without having to create intermediate variables for `resid` and `squared_error`). Check that you get the same result.\n",
    "\n",
    "{{% /task %}}\n",
    "\n",
    "Notes:\n",
    "\n",
    "- Recall that Python's exponentiation operator is `**`.\n",
    "- PyTorch tensors also have a `.pow()` method. So you might see `.pow(2)`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed68b87",
   "metadata": {},
   "source": [
    "### Dot products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260bba59",
   "metadata": {},
   "source": [
    "Recall that we can make a line by an expression like `y = w*x + b`. (Some of you may remember *mx+b* , but we'll use *w* for the *weight(s)* instead.)\n",
    "\n",
    "That's a multiplication followed by a sum. We can extend that to lots of *x*'s, each of which needs a corresponding *w*:\n",
    "\n",
    "`y = w1*x1 + w2*x2 + ... + wN*xN + b`\n",
    "\n",
    "For simplicity, let's start by ignoring the `b`ias.  So we're left with\n",
    "\n",
    "`y = w1*x1 + w2*x2 + ... + wN*xN`\n",
    "\n",
    "that is, multiply each number in `w` by its corresponding number in `x` and add up the results: `sum(w[i] * x[i] for i in range(N))`.\n",
    "\n",
    "The result is called a *dot product*, and is one of the fundamental operations in linear algebra. At this point you don't need to understand all the linear algebra part of this, we're just implementing a common calculation.\n",
    "\n",
    "Let's do that in Python, and then Torch. To start, let's make a `w`eights and an `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d290d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tensor([-2.0, 1.0])\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9487b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tensor([1.5, 2.0])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbafdfed",
   "metadata": {},
   "source": [
    "The shapes of `w` and `x` must match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbce88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(w)\n",
    "assert N == len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38d0106",
   "metadata": {},
   "source": [
    "#### `for` loop approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6255210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_loop(w, x):\n",
    "    return 0.0 # FIXME\n",
    "dot_loop(w, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3414e8d",
   "metadata": {},
   "source": [
    "#### Torch Elementwise Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c901a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_ops(w, x):\n",
    "    return 0.0\n",
    "dot_ops(w, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49040893",
   "metadata": {},
   "source": [
    "## Linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f44bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(weights, bias, x):\n",
    "    return 0.0 # FIXME\n",
    "linear(w, 1.0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe09e53f",
   "metadata": {},
   "source": [
    "### Linear layer, Module-style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b739211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = ...\n",
    "        self.bias = ...\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return ...\n",
    "\n",
    "layer = Linear(weights=w, bias=1.0)\n",
    "layer.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3a1db8",
   "metadata": {},
   "source": [
    "## Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca6f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tensor([3.14, 1.59, 2.65])\n",
    "y_pred = tensor([2.71, 8.28, 1.83])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
