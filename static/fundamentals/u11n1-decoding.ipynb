{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9O35L1Bcd2rU"
   },
   "source": [
    "## Translation as Language Modeling\n",
    "\n",
    "Goals:\n",
    "\n",
    "* Practice getting data into and out of a language model.\n",
    "    * embeddings (input and output)\n",
    "    * logits for next words\n",
    "    * cross-entropy loss\n",
    "* Explore different methods of *decoding* for sequence generation\n",
    "* Explain how data flows between the encoder and decoder in a sequence-to-sequence model\n",
    "* Interpret *attention weights*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZNlX5Kbe3_R"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyDS4bzee5Rq"
   },
   "source": [
    "Install libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPTTlj3uW9FZ"
   },
   "outputs": [],
   "source": [
    "#%pip install -q datasets transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHHEJjq3fCqC"
   },
   "source": [
    "Import PyTorch and the HuggingFace Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ueA5Mqomo95h"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uegQ3XfbfNGS"
   },
   "source": [
    "Load a [Marian Machine Translation](https://huggingface.co/transformers/model_doc/marian.html) model.\n",
    "\n",
    "Specifically, we're using one that was trained on the OPUS corpus (`opus-mt`) to translate text in any romance language (`ROMANCE`) to English (`en`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227,
     "referenced_widgets": [
      "103cf625273e437a861ffcb276b60417",
      "fb4ee74914264c6295cdb25de47bedf7",
      "0d4ce5f06380422ab6993cfeca1f4d0e",
      "37f25e16709041b9bd15f6e28e1a67aa",
      "aa08d2af16854091bcd504138d5a23ed",
      "1165bfd84f33441ba7b826c6ea738404",
      "022e7d42a69349579a0c318fe8ce3456",
      "bda12f7ee05547aa89bc8cd9e6ebe766",
      "1bafd8ae8bb34371a30c5e1a98aa2c7f",
      "d31617a865c04430a4fd9d89fd463c9d",
      "743458816ed4438e95a350acb77a1ed9",
      "92d425ab0b74433ca5f58ddbca5b0aac",
      "b79f425bff5145539e6cbc79f057a00b",
      "ff65b9819451494f8e0fcdc18845a6eb",
      "e50c7f1557c7416bb46332bea35a2e4c",
      "5c23d16848cf428db6e5ab919d7f330d",
      "4e565a160f0140d8be3a3e7d3a71d5a6",
      "64e238340e814e8ba68f4eb3e3f92236",
      "a89dea4aedfd47d1af3790947f235592",
      "bfb36fd29cf149878d3da9c52f984b2c",
      "7d5b4f7df1b349548c7018f7ab60eb19",
      "c3ddaca006bd40598bd3d2f577d6c433",
      "3379efa33ed84b8fa5198955c2763c34",
      "66c9f40b5445496082b7371129789db3",
      "361c9d10105f4191a2e66ecb6f19d49a",
      "2a7b28600cb8438790e1e05bedcd02ab",
      "eafa8f3a00e548bda56460c1b242ba0e",
      "6b5ae1bf6251431b826cf4a1574ccc42",
      "a9410f2983f343c8b9c55096761df076",
      "f29ce9f71d0e4791a7eaeab511038c83",
      "9d2d00fcf982463382d861123974d1bf",
      "9e9e568d07ad4416ab3a4a13ef0c439f",
      "c296ce0fe60d40eaac1b3e3d92efa7ae",
      "6241288e90004dceb56498272d76af14",
      "edffeffee7cf4da7aa006609bd01b306",
      "3f2baff07c5e42478d49dc996fdc8222",
      "113d9f5677e54d6587fa25b709e331af",
      "00130d1e77a24e4cbf2b0b5263f6fdfc",
      "080060ec55934c839e97d0e200588444",
      "7994b7846d7c4eeda6d32209890736e6",
      "8c6d216b69ab41e29e37989fbaaf49a8",
      "79cbcf958b9b48b1af2c12ad9d505570",
      "5b3740d3d7664df8bedd145dfa268e45",
      "ceb7b9a6c7f147d4820b58e816a6d5b9",
      "cc5fdcf2464a4d0d83463871fbdf9c3d",
      "60dd1e7911e944fd94b8c58e1e39bc2a",
      "f73b432f420e4d4cbe33202f409dc593",
      "229e90c092c342eba32ec90ad5731b18",
      "d3e0b17011e6438aa78c7e738701e114",
      "1ba578a93bdc44099e0bdfe4ae5b0150",
      "01e4b682cc1546a8a2d564ba2332ea55",
      "35943f215e39476baada398fef03bd60",
      "c832f702d0524c9285afe01638d981dc",
      "01e02eed4ae04ba9b7b5e9e27aa73863",
      "06215b9e85f04655917b8292611edaae",
      "a24f572c83154528ad3eaaa6dfd4f040",
      "c7ff9352f2ac46118a30e1f07834ccf3",
      "6059ed2107324e78bafddbd3c06a14c0",
      "a44a6fde5ea34c459b6d921cb65f49bc",
      "1556ceeef7d5411e86c1896505513e4d",
      "0076051ba3aa418c9027268a551b2171",
      "6bed6df654ef420691de365de4969aef",
      "a58eb022e4c64403af89889c29f31242",
      "290e86a6a8b24c30b6936eea422b9fcb",
      "34346a01cb6a45339cf7d54b328514f0",
      "ba7d1cb1be3e41ef87c328b818627a72"
     ]
    },
    "id": "oXDwrEx8nTj7",
    "outputId": "3a2eb85f-274b-4bac-ec63-2181ee433b95"
   },
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "model_name = 'Helsinki-NLP/opus-mt-ROMANCE-en'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "print(f\"The model has {model.num_parameters():,d} parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NDajxTdhLAp"
   },
   "source": [
    "Finally, these wrappers will make the code below easier to understand (you should completely ignore them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEgzyR1ShKjr"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from transformers.models.marian.modeling_marian import shift_tokens_right\n",
    "prepend_start_token = partial(\n",
    "    shift_tokens_right,\n",
    "    pad_token_id = model.config.pad_token_id, decoder_start_token_id = model.config.decoder_start_token_id)\n",
    "encoder = model.get_encoder()\n",
    "decoder = model.get_decoder()\n",
    "encoder.forward = partial(encoder.forward, output_attentions=True, output_hidden_states=True)\n",
    "decoder.forward = partial(decoder.forward, output_attentions=True, output_hidden_states=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4D35Lumfqsr"
   },
   "source": [
    "## Warm-up\n",
    "\n",
    "Let's practice with the tokenizer. This should be mostly review, but we'll do it in the way that the HuggingFace docs do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zZL-PQ5BosDz",
    "outputId": "86f75716-037c-4817-a7d3-0aea294b665b"
   },
   "outputs": [],
   "source": [
    "spanish_text = \"Yo les doy vida eterna.\"\n",
    "spanish_batch = tokenizer(spanish_text, return_tensors='pt', padding=True).to(device)\n",
    "spanish_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKUhKBeAgVXf"
   },
   "source": [
    "Since we're only translating one sentence, we can ignore `attention_mask` (which just helps ignore padding tokens) and the extra initial dimension of the `input_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fIh9u_gEsbPm",
    "outputId": "6d52f3ed-c90e-4173-a52e-857095de9aa8"
   },
   "outputs": [],
   "source": [
    "input_ids = spanish_batch.input_ids\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "exn_PoiHgjS-",
    "outputId": "1be82555-e860-4adf-c2fa-b61e37626cd4"
   },
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Khl1_BY3gG9W"
   },
   "source": [
    "Now let's ask the model to generate a translation. Lots of magic happens here; we'll peel back the layers shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Q9a07LDnw-y",
    "outputId": "f6b24b3f-9b94-4891-9a60-865f89b33ce8"
   },
   "outputs": [],
   "source": [
    "translated = model.generate(input_ids = input_ids, num_beams=1, do_sample=False)\n",
    "translated.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BTxglJpgm4Z"
   },
   "source": [
    "Decode the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "IMz_2_OjrzHg",
    "outputId": "5ec05b53-a6ca-487b-e5b2-f9ab16633de5"
   },
   "outputs": [],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    english_text = tokenizer.decode(translated[0])\n",
    "english_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generate` method can use several different algorithms under the hood. Let's see how each of them behaves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_for_sequences(logits, targets):\n",
    "    '''\n",
    "    Standard F.cross_entropy doesn't handle multiple sequences consistently.\n",
    "\n",
    "    This is a slow approach to get the correct sequences. (Faster would be to not reduce and multiply by (targets >= 0).sum(axis=1).)\n",
    "    '''\n",
    "    return [\n",
    "        F.cross_entropy(inp, tgt) for inp, tgt in zip(logits.unbind(), targets.unbind())\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_params(input_ids, **kwargs):\n",
    "    # Generate translations. Tell `generate` to give us the logits (which they call \"scores\")\n",
    "    translations = model.generate(input_ids = input_ids, return_dict_in_generate=True, output_scores=True, **kwargs)\n",
    "\n",
    "    # Recompute the cross-entropy (some `generate` outptus give us this, others don't, so we have to recompute).\n",
    "    logprobs = cross_entropy_for_sequences(translations.scores, translations.sequences)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        return pd.DataFrame({\n",
    "            'sentence': tokenizer.batch_decode(translations.sequences),\n",
    "            'logprobs': logprobs\n",
    "        })\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFgxHsTruVLi"
   },
   "outputs": [],
   "source": [
    "#with tokenizer.as_target_tokenizer():\n",
    "#    english_batch = tokenizer(english_text, return_tensors='pt', padding=True).to(device)\n",
    "#decoder_input_ids = torch.tensor([model.config.decoder_start_token_id]).unsqueeze(0)\n",
    "#outputs = model(input_ids=spanish_batch.input_ids, decoder_input_ids=decoder_input_ids)\n",
    "#outputs.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ap6YNNpG9hX"
   },
   "source": [
    "## Predict Next Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "jKd1cWlqAWJE",
    "outputId": "f4e3044a-53cf-4926-e1d2-c2fb879335d9"
   },
   "outputs": [],
   "source": [
    "def predict_next_token(source_input_ids, decoded_so_far=[], k=5):\n",
    "    decoder_input_ids = torch.tensor([model.config.decoder_start_token_id] + decoded_so_far).unsqueeze(0).to(device)\n",
    "    assert input_ids.shape[0] == 1\n",
    "    with torch.no_grad(): # This tells PyTorch we don't need it to compute gradients for us.\n",
    "        model_output = model(input_ids = source_input_ids, decoder_input_ids=decoder_input_ids)\n",
    "    last_token_logits = model_output.logits[0, -1].cpu()\n",
    "    assert len(last_token_logits.shape) == 1\n",
    "    most_likely_tokens = last_token_logits.topk(k)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        probs = most_likely_tokens.values.softmax(dim=0)\n",
    "        return pd.DataFrame({\n",
    "            'token': [tokenizer.decode(token_id) for token_id in most_likely_tokens.indices],\n",
    "            'id': most_likely_tokens.indices,\n",
    "            'probability': probs,\n",
    "            'logprob': probs.log(),\n",
    "            'cumulative probability': probs.cumsum(0)\n",
    "        })\n",
    "\n",
    "\n",
    "print(-0.028718 + -0.083301, \"I give\")\n",
    "predict_next_token(spanish_batch.input_ids, [20, 685])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring a candidate translation\n",
    "\n",
    "The model gives us conditional probabilities of each word: `P(word_i | src, word_1, word_2, ..., word_{i-1})`. You should recognize these as the softmax \n",
    "\n",
    "We can use those to compute the probability of a complete translation by multiplying the conditional probabilities:\n",
    "\n",
    "TODO\n",
    "\n",
    "Those underflow, so we actually use the logs TODO\n",
    "\n",
    "which are conveniently the cross-entropy loss values of each token. TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ky84KWXGhcnj"
   },
   "source": [
    "First, let's look at the *loss* that the model gives. We'll compare the correct translation with an incorrect one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7c5J26Dt6jZ",
    "outputId": "b911d599-0970-46ca-c2c8-7b259aef49ec"
   },
   "outputs": [],
   "source": [
    "def tokenize_target_sentence(sentence):\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        return tokenizer(sentence, return_tensors='pt', padding=True).to(device)\n",
    "wrong_target_batch = tokenize_target_sentence(\"I give them eternal death.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vh9Px6rtRgDr"
   },
   "source": [
    "Let's **run a forward pass through the full model** (encoder and decoder) with the complete candidate translation. First, the correct translation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gYaAPmpRSX5S",
    "outputId": "2bafd185-724c-4155-b232-1e422d43504e"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad() # We don't need to compute gradients \n",
    "def get_logprob_of_translation(src_ids, tgt_ids):\n",
    "    model_outputs = model(\n",
    "        input_ids = src_ids,\n",
    "        labels = tgt_ids\n",
    "    )\n",
    "    return model_outputs.loss # TODO: multiply by num tokens? Replace by manually doing cross_entropy_loss?\n",
    "get_logprob_of_translation(spanish_batch.input_ids, correct_batch.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWbjU4Z4h8Ju"
   },
   "source": [
    "Now (**your turn**) the incorrect translation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2GRCRvyMW7u7",
    "outputId": "ca48ec81-cf87-49dd-cbc9-508e4e4ec99c"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-suuWxoskRW"
   },
   "source": [
    "## Dig In!\n",
    "\n",
    "Ok now how did it do that?\n",
    "\n",
    "You may find it helpful to have the [documentation for the MarianMT model in HuggingFace Transformers](https://huggingface.co/transformers/model_doc/marian.html#transformers.MarianMTModel.forward) open. But you can do all of this without referring to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHyWeU3SiHdm"
   },
   "source": [
    "### The guts of the model\n",
    "\n",
    "I've ripped out all the plumbing code and things you only need in special situations to just show the guts of the model below. **Study this code carefully** with the help of the questions below it. **Add comments** to describe what each line does. Include, where applicable, the *shape* of the tensors involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2U_cPjWunaTA",
    "outputId": "4db9d997-8078-4161-a1f1-2a339e8f40b3"
   },
   "outputs": [],
   "source": [
    "encoder_input_ids = spanish_batch.input_ids\n",
    "target_ids = english_batch.input_ids\n",
    "decoder_input_ids = prepend_start_token(target_ids)\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder_outputs = encoder(input_ids = encoder_input_ids)\n",
    "    # (Aside: an alternative to the above)\n",
    "    # encoder_input_embeddings = encoder.embed_tokens(encoder_input_ids) * encoder.embed_scale\n",
    "    # encoder_outputs = encoder(inputs_embeds = encoder_input_embeddings)\n",
    "\n",
    "    decoder_outputs = decoder(\n",
    "        input_ids = decoder_input_ids,\n",
    "        encoder_hidden_states = encoder_outputs.last_hidden_state\n",
    "    )\n",
    "\n",
    "    output_embedding = decoder_outputs.last_hidden_state\n",
    "    token_embeddings = model.lm_head.weight\n",
    "    logits = output_embedding @ token_embeddings.t()\n",
    "    logits += model.final_logits_bias\n",
    "\n",
    "    # ignore the batch dimension.\n",
    "    logits = logits[0]\n",
    "\n",
    "nlls_of_correct_tokens = F.cross_entropy(logits, target_ids[0], reduction='none')\n",
    "nlls_of_correct_tokens.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4b6mUhLkier6"
   },
   "source": [
    "**Explain `logits.shape`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrqgcDuvZXig",
    "outputId": "0c085f29-33fd-46a1-9aa3-f4f0a1a735dd"
   },
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AJavHl0ik-D"
   },
   "source": [
    "*your narrative answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F7lk2zRdZapr",
    "outputId": "88002490-9be6-4d82-e53c-8874c5f571f4"
   },
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(logits.argmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kpcAYSzWaPKw",
    "outputId": "a27a51ce-511f-4cb7-d319-42443b627539"
   },
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(target_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPojgCC_Y4w6"
   },
   "source": [
    "**What tensor contains all of the information from the Spanish sentence that is used to generate the English sentence? Explain each element of the shape of that tensor.**\n",
    "\n",
    "(The leading \"1\" is the batch dimension; you can ignore this unless you're translating multiple sentence simultaneously.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7POLGH8Ni_a"
   },
   "source": [
    "**What is the \"shape\" of this model?** Specifically:\n",
    "\n",
    "1. What is the dimensionality of the hidden vectors it uses to represent everything? (How does this relate to the dimensionality of the token embeddings?)\n",
    "2. How many internal layers does the model have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h3pIYTlgWXjy",
    "outputId": "d1d2ed3a-d7e2-4f37-a902-c2929c2a06f7"
   },
   "outputs": [],
   "source": [
    "encoder_outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NAL9E2RVvY3f",
    "outputId": "bf4c9f0e-4e65-4924-84a7-d1504df25b91"
   },
   "outputs": [],
   "source": [
    "model.config.num_hidden_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoQm5WSMvd6a"
   },
   "source": [
    "## Visualize attentions\n",
    "\n",
    "Read these as: the row token looks at the column token.\n",
    "\n",
    "There are actually 8 attention heads for each of the 6 layers, so to visualize simply, we take the mean over the attention weights (which are all positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtC2auFF-bVr",
    "outputId": "21f1ee4e-954e-4389-eb3a-1d5d9d8f0c02"
   },
   "outputs": [],
   "source": [
    "decoder_outputs.cross_attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "OXXjXCbGvTd7",
    "outputId": "a2a94beb-02a3-4adc-fa4d-4865004d5622"
   },
   "outputs": [],
   "source": [
    "layer = 1\n",
    "plt.pcolormesh(decoder_outputs.cross_attentions[layer][0].mean(dim=0).cpu().numpy())\n",
    "plt.title(f\"Cross-Attention Weights for layer {layer} (avg over all {model.config.num_attention_heads} heads)\")\n",
    "plt.xticks(torch.arange(8)+.5, tokenizer.convert_ids_to_tokens(encoder_input_ids[0]))\n",
    "plt.yticks(torch.arange(7)+.5, tokenizer.convert_ids_to_tokens(decoder_input_ids[0]))\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "QISHPryfjGLP",
    "outputId": "3eabca9f-18dc-4227-b888-b2ae70fcfe1d"
   },
   "outputs": [],
   "source": [
    "layer = -1\n",
    "plt.pcolormesh(encoder_outputs.attentions[layer][0].mean(dim=0).cpu().numpy())\n",
    "plt.title(f\"Encoder Self-Attention Weights for layer {layer} (avg over all {model.config.num_attention_heads} heads)\")\n",
    "plt.xticks(torch.arange(8)+.5, tokenizer.convert_ids_to_tokens(encoder_input_ids[0]))\n",
    "plt.yticks(torch.arange(8)+.5, tokenizer.convert_ids_to_tokens(encoder_input_ids[0]))\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "K3Vgm5LDH3z7",
    "outputId": "a3a96f93-45dc-46c1-b7e6-dab6b9bb19ec"
   },
   "outputs": [],
   "source": [
    "layer = 0\n",
    "plt.pcolormesh(decoder_outputs.attentions[layer][0].mean(dim=0).cpu().numpy())\n",
    "plt.title(f\"Decoder Self-Attention Weights for layer {layer} (avg over all {model.config.num_attention_heads} heads)\")\n",
    "plt.xticks(torch.arange(7)+.5, tokenizer.convert_ids_to_tokens(decoder_input_ids[0]))\n",
    "plt.yticks(torch.arange(7)+.5, tokenizer.convert_ids_to_tokens(decoder_input_ids[0]))\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feICCpckNMyE"
   },
   "source": [
    "## Similarity\n",
    "\n",
    "Notice that the last step of the model is a dot product with all the token embeddings. Recall that a dot product is a measure of similarity. Let's look at similarity in embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Gm31JRPPGnb"
   },
   "outputs": [],
   "source": [
    "normalized_token_embeddings = token_embeddings / token_embeddings.norm(p=2, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sjLZvKHRNMqY",
    "outputId": "0518ea51-79d9-4440-c3d6-a27a57e425d7"
   },
   "outputs": [],
   "source": [
    "query_word = \"London\"\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    query_ids = tokenizer.encode(query_word, add_special_tokens=False)\n",
    "print(query_ids)\n",
    "query = token_embeddings[query_ids].mean(dim=0)\n",
    "similarities = query @ normalized_token_embeddings.t()\n",
    "most_similar_indices = similarities.topk(50).indices\n",
    "tokenizer.convert_ids_to_tokens(most_similar_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwmyxQSWS23W"
   },
   "source": [
    "**Your turn**: now, take query vectors from the `output_embeddings` that were calculated above and find the most similar token embeddings.\n",
    "\n",
    "Compare the results with the translation output you saw from the model earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vU1LuB7LSnWx",
    "outputId": "4b712d7f-1971-4895-882b-bf99fb1b7d68"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Na17o8LBNMi1"
   },
   "source": [
    "## The Logit Lens (optional)\n",
    "\n",
    "This is an exploration inspired by [this article](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens). Intuition: the Transformer iteratively refines a guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5Fe4bPBonSp"
   },
   "outputs": [],
   "source": [
    "# http://stephantul.github.io/python/pytorch/2020/09/18/fast_topk/\n",
    "def get_ranks(values, indices):\n",
    "    targets = values[range(len(values)), indices]\n",
    "    return (values > targets[:, None]).long().sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GllA5Shlk6AC",
    "outputId": "bdf0cdc1-b91a-4b3e-ac18-c4a18b08c819"
   },
   "outputs": [],
   "source": [
    "ranks = []\n",
    "print(tokenizer.convert_ids_to_tokens(decoder_input_ids[0]))\n",
    "for hidden in decoder_outputs.hidden_states[1:]:\n",
    "    x = model.lm_head(hidden)[0]\n",
    "    print(tokenizer.convert_ids_to_tokens(x.argmax(dim=1)))\n",
    "    ranks.append(get_ranks(x, target_ids[0]))\n",
    "torch.stack(ranks[::-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wpORbu30m6Nx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Translation as Language Modeling using Transformers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
