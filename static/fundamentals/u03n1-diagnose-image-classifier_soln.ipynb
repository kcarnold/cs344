{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Diagnostics\n",
    "\n",
    "Task: plot a confusion matrix, find images that were misclassified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do not need to read or modify the code in this section to successfully complete this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import fastai code.\n",
    "from fastai.vision.all import *\n",
    "\n",
    "# Set a seed for reproducibility.\n",
    "set_seed(0, reproducible=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monkey-patch `plot_top_losses` because of a bug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_top_losses(self, k, largest=True, **kwargs):\n",
    "    losses,idx = self.top_losses(k, largest)\n",
    "    if not isinstance(self.inputs, tuple): self.inputs = (self.inputs,)\n",
    "    if isinstance(self.inputs[0], Tensor): inps = tuple(o[idx] for o in self.inputs)\n",
    "    else: inps = self.dl.create_batch(self.dl.before_batch([tuple(o[i] for o in self.inputs) for i in idx]))\n",
    "    b = inps + tuple(o[idx] for o in (self.targs if is_listy(self.targs) else (self.targs,)))\n",
    "    x,y,its = self.dl._pre_show_batch(b, max_n=k)\n",
    "    b_out = inps + tuple(o[idx] for o in (self.decoded if is_listy(self.decoded) else (self.decoded,)))\n",
    "    x1,y1,outs = self.dl._pre_show_batch(b_out, max_n=k)\n",
    "    if its is not None:\n",
    "        plot_top_losses(x, y, its, outs.itemgot(slice(len(inps), None)), self.preds[idx], losses, **kwargs)\n",
    "ClassificationInterpretation.plot_top_losses = _plot_top_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.PETS)/'images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = get_image_files(path).sorted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cat images have filenames that start with a capital letter.\n",
    "def is_cat(filename):\n",
    "    return filename[0].isupper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally corrupt some of the image labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLIP_PROB = 0.0 # <--AFTER FINISHING, try setting this to 0.25 to play with detecting mislabeled images\n",
    "correct_labels = [is_cat(path.name) for path in image_files]\n",
    "corrupted_labels = [\n",
    "    not correct_label if random.random() < FLIP_PROB else correct_label\n",
    "    for correct_label in correct_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many labels are still correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(\n",
    "    correct_label == corrupted_label\n",
    "    for correct_label, corrupted_label in zip(correct_labels, corrupted_labels)\n",
    ") / len(correct_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = ImageDataLoaders.from_lists(\n",
    "    path=path, fnames=image_files, labels=corrupted_labels,\n",
    "    valid_pct=0.2,\n",
    "    seed=42,\n",
    "    item_tfms=Resize(224)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(\n",
    "    dls=dataloaders,\n",
    "    arch=resnet18,\n",
    "    metrics=accuracy\n",
    ")\n",
    "learn.fine_tune(epochs=4)\n",
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've given you a classifier (the `learn` object). It makes a few mistakes; can you find them?\n",
    "\n",
    "> The code above provides a way to *corrupt* some of the labels before training. For the purposes of this assignment, the corruption machinery is turned off. But you might find it enlightening to re-enable it and see how a classifier handles mislabeled data. But wait until *after you finish this assignment*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow these steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Show one batch from each of the training and validation sets. (Find the `DataLoader` objects at `dataloaders.train` and `dataloaders.valid`; each of them has a `.show_batch()` method.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "dataloaders.train.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "dataloaders.valid.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compute the *accuracy* and *error rate* of this classifier on the validation set (`accuracy(interp.preds, interp.targs)`). Check that this number matches the last accuracy figure reported while training above. Multiply this by the number of images in the validation set to give the actual number of misclassified images.\n",
    "\n",
    "*Hints*:\n",
    "\n",
    "- You may need `WHATEVER.item()` to get a plain number instead of a `Tensor`.\n",
    "- `DataLoader`s have a `.n` attribute that gives the number of images in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "print(\"Accuracy:\", accuracy(interp.preds, interp.targs).item())\n",
    "print(\"Error rate: \", error_rate(interp.preds, interp.targs).item())\n",
    "print(f\"Number of images incorrect: {round(error_rate(interp.preds, interp.targs).item() * dataloaders.valid.n)} out of {dataloaders.valid.n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Plot the confusion matrix on the validation set (see chapter 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compute the accuracy on the *training* set. (Since \"dataset 0\" is the training set and \"dataset 1\" is the validation set, we can use `interp_train = ClassificationInterpretation.from_learner(learn, ds_idx=0)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_train = ClassificationInterpretation.from_learner(learn, ds_idx=0)\n",
    "# your code here\n",
    "print(\"Accuracy:\", accuracy(interp_train.preds, interp_train.targs).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Plot the top 12 losses in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_top_losses(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **How many dogs in the validation set were misclassified as cats? Vice versa?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X out of XX images were incorrectly labeled \"cat\".\n",
    "\n",
    "Y out of YY images were incorrectly labeled \"dog\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **If we had only looked at the accuracy on the training set, would we have *overestimated* or *underestimated* how well the classifier would have performed on the validation set? By how much?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Examine the top losses plot.\n",
    "  1. Explain what the four things above each image mean.\n",
    "  2. Explain why some correctly classified images appear in the \"top losses\".\n",
    "  3. What is the relationship between \"loss\" and \"probability\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
